# -*- coding: utf-8 -*-
"""2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KYBMvVqXxD8-piywFm0_ElhZRyPHx03i
"""

import numpy as np 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import r2_score,mean_absolute_error as mae
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV,cross_val_score
from sklearn.preprocessing import StandardScaler

import nltk
nltk.download('stopwords')
nltk.download('punkt')

import re
from nltk.tokenize import word_tokenize

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

df = pd.read_csv('/content/DA-AI-ML-interview-assignment-Data (1).csv')

df.head()

df.corr()

def convert1(s):
  l = []
  s = list(s)
  for i in s:
    if 'followers' in i:
      i = i.replace(" followers","")
    else:
      i = i  
    if 'k' not in i:
      u = i.replace("," , "")
      u = int(u)
    elif 'k' in i:
      u = float(i[0:len(i) - 1]) * 1000
    else:
      u = i  
    l.append(u)
  return l 



def data_pre(text):
  text = str(text).lower()
  text = re.sub(r"https\S+","",text,flags=re.MULTILINE)

  text = re.sub(r'\@w+|\#',"",text)

  text = re.sub(r'[^\w\s]' , "", text)

  text_tokens = word_tokenize(text)

  filtered_text = [w for w in text_tokens if not w in stop_words]

  f = " ".join(filtered_text)

  y = []

  for i in list(f.split(" ")):
    y.append(stemmer.stem(i))

  return " ".join(y)

df['num_ratings'] = convert1(df['num_ratings'])
df['num_reviews'] = convert1(df['num_reviews'])
df['num_followers'] = convert1(df['num_followers'])

df['num_ratings'] = df['num_ratings'] / 1000

df.drop(['Unnamed: 0'] , axis = 1 , inplace = True)

df.corr()

plt_1 = plt.figure(figsize=(12, 6))

sns.barplot(x = df['genre'] , y = df['rating'])
plt.show()

"""as we already know, there is no co relation between rating and genre

so we can remove the genre feature
"""

df.drop(['genre'] , axis = 1 , inplace = True)

plt_1 = plt.figure(figsize=(12, 6))

sns.distplot(df['rating'])
plt.show()

plt_1 = plt.figure(figsize=(12, 6))

sns.distplot(df['num_ratings'])
plt.show()

plt_1 = plt.figure(figsize=(12, 6))

sns.distplot(df['num_reviews'])
plt.show()

plt_1 = plt.figure(figsize=(12, 6))

sns.distplot(df['num_followers'])
plt.show()

df.drop(['num_ratings'] , axis = 1 , inplace = True)

df.head()

plt_1 = plt.figure(figsize=(12, 6))

sns.boxplot(x = df['num_reviews'])
plt.show()

plt_1 = plt.figure(figsize=(12, 6))

sns.boxplot(x = df['num_followers'])
plt.show()

plt_1 = plt.figure(figsize=(12, 6))

sns.boxplot(x = df['rating'])
plt.show()

df['num_reviews'].skew()

df['rating'].skew()

def outlier(s):

  q1 = df[s].quantile(0.25)
  q2 = df[s].quantile(0.50)
  q3 = df[s].quantile(0.75)
  q4 = df[s].quantile(1.0)

  iqr = q3 - q1

  lr = q1 - (1.5) * iqr
  ur = q3 + (1.5) * iqr

  return len(df[df[s] < lr]) + len(df[df[s] > ur])

l = ['rating',	'num_reviews' ,	'num_followers']

print("Percentage of Outliers")
print()
for i in l:
  print(i , outlier(i) * 100/ len(df))

import warnings
warnings.filterwarnings('ignore')


dff = df[['title', 'name', 'num_reviews','num_followers', 'synopsis']]
dff['tags'] = dff['title'] + " " + dff['name'] + " " + dff['synopsis']

df2 = dff[['num_reviews' , 'num_followers', 'tags']]

df2['tags'] = df2.tags.apply(data_pre)

X = df2

X = pd.concat([X , df['rating']] , axis = 1)

!pip install -U pip setuptools wheel
!pip install -U spacy
!python -m spacy download en_core_web_lg

X

import spacy
nlp = spacy.load("en_core_web_lg")

X['vector'] = X['tags'].apply(lambda tags: nlp(tags).vector)

xm = np.stack(X['vector'])

P = pd.DataFrame(xm)
R = pd.concat([X.drop(['tags' , 'vector'] , axis = 1) , P] , axis = 1)

z = R

A_train2, A_test2, b_train2, b_test2 = train_test_split(z.drop(['rating'] , axis = 1),z['rating'], test_size=0.20, random_state=42 ,shuffle = True)

def build3_model(model):
  model.fit(A_train2 , b_train2)
  b_pred2 = model.predict(A_test2)
  
  print("r2_score  :" , r2_score(b_test2 , b_pred2))
  print("mean squared error  :" , mse(b_test2 , b_pred2)) 
  print("root mean squared error  :" , mse(b_test2 , b_pred2) ** (0.5))
  print("best parameters  :" , model.best_params_)
  print()
  print()

import warnings
warnings.simplefilter("ignore")


model1 = ElasticNet()
params = {
    'alpha':[1 , 0.5 , 0.05 , 0.7],
    'l1_ratio':[0.1 , 0.001 , 0.005 , 0.2 , 0.002]
}

grid = GridSearchCV(estimator = model1 , param_grid = params , scoring = 'r2' , n_jobs = -1 , cv = 5)

print(cross_val_score(grid , A_train2 , b_train2 , cv = 5 , scoring = 'r2').mean)

build3_model(grid)

import warnings
warnings.simplefilter("ignore")

model2 = RandomForestRegressor()

parms = {'bootstrap': [True, False],
 'max_depth': [10],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2,3],
 'min_samples_split': [2, 5,],
 'n_estimators': [10,20]}


grid1 = GridSearchCV(estimator = model2 , param_grid = parms , scoring = 'r2' , n_jobs = -1 , cv = 5)

print(cross_val_score(grid , A_train2 , b_train2 , cv = 5 , scoring = 'r2').mean)

build3_model(grid1)

t = df2.drop(['vector'] , axis = 1)

tf_vec = TfidfVectorizer(min_df = 5)
tf = tf_vec.fit_transform(t['tags'].values)

a1 = tf.toarray()
d1 = pd.DataFrame(a1)

k = pd.concat([d1,t.drop(['tags'] , axis = 1)] , axis = 1)



A_train1, A_test1, b_train1, b_test1 = train_test_split(k,df['rating'], test_size=0.20, random_state=42 ,shuffle = True)

def build4_model(model):
  model.fit(A_train1 , b_train1)
  b_pred1 = model.predict(A_test1)
  
  print("r2_score  :" , r2_score(b_test1 , b_pred1))
  print("mean squared error  :" , mse(b_test1 , b_pred1)) 
  print("root mean squared error  :" , mse(b_test1 , b_pred1) ** (0.5))
  print("best parameters  :" , model.best_params_)
  print()
  print()

import warnings
warnings.simplefilter("ignore")


modelw = ElasticNet()
params = {
    'alpha':[1 , 0.5 , 0.05 , 0.7],
    'l1_ratio':[0.1 , 0.001 , 0.005 , 0.2 , 0.002]
}

gridw = GridSearchCV(estimator = modelw , param_grid = params , scoring = 'r2' , n_jobs = -1 , cv = 5)

print(cross_val_score(gridw , A_train1 , b_train1 , cv = 5 , scoring = 'r2').mean)

build4_model(gridw)

"""Feature transformation"""

R

def transform(s):
  return np.log(s)

transform(R['num_reviews'])

plt_1 = plt.figure(figsize=(12, 6))

sns.distplot(transform(R['num_reviews']))
plt.show()

plt_1 = plt.figure(figsize=(12, 6))

sns.distplot(transform(R['num_followers']))
plt.show()

R['num_reviews'] = transform(R['num_reviews'])
R['num_followers'] = transform(R['num_followers'])

p = R

A_train3, A_test3, b_train3, b_test3 = train_test_split(p.drop(['rating'] , axis = 1),p['rating'], test_size=0.20, random_state=42 ,shuffle = True)

A_train3

scaler = StandardScaler()
A_train3 = scaler.fit_transform(A_train3)
A_test3 = scaler.transform(A_test3)

def build5_model(model):
  model.fit(A_train3 , b_train3)
  b_pred3 = model.predict(A_test3)
  
  print("r2_score  :" , r2_score(b_test3 , b_pred3))
  print("mean squared error  :" , mse(b_test3 , b_pred3)) 
  print("root mean squared error  :" , mse(b_test3 , b_pred3) ** (0.5))
  print("best parameters  :" , model.best_params_)
  print()
  print()

import warnings
warnings.simplefilter("ignore")


modelp = ElasticNet()
params = {
    'alpha':[1 , 0.5 , 0.05 , 0.7],
    'l1_ratio':[0.1 , 0.001 , 0.005 , 0.2 , 0.002]
}

gridw = GridSearchCV(estimator = modelp , param_grid = params , scoring = 'r2' , n_jobs = -1 , cv = 5)

print(cross_val_score(gridw , A_train3 , b_train3 , cv = 5 , scoring = 'r2').mean)

build5_model(gridw)





