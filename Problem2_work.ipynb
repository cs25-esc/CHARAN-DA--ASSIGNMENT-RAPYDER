{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fZBf2x7lfFZ7"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import r2_score,mean_absolute_error as mae\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMia3plbfdVC",
        "outputId": "4f6c6ab6-e763-46ea-8b5c-032ba3144c8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "6pKwl12xfdXn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert1(s):\n",
        "  l = []\n",
        "  s = list(s)\n",
        "  for i in s:\n",
        "    if 'followers' in i:\n",
        "      i = i.replace(\" followers\",\"\")\n",
        "    else:\n",
        "      i = i  \n",
        "    if 'k' not in i:\n",
        "      u = i.replace(\",\" , \"\")\n",
        "      u = int(u)\n",
        "    elif 'k' in i:\n",
        "      u = float(i[0:len(i) - 1]) * 1000\n",
        "    else:\n",
        "      u = i  \n",
        "    l.append(u)\n",
        "  return l \n",
        "\n",
        "\n",
        "\n",
        "def data_pre(text):\n",
        "  text = str(text).lower()\n",
        "  text = re.sub(r\"https\\S+\",\"\",text,flags=re.MULTILINE)\n",
        "\n",
        "  text = re.sub(r'\\@w+|\\#',\"\",text)\n",
        "\n",
        "  text = re.sub(r'[^\\w\\s]' , \"\", text)\n",
        "\n",
        "  text_tokens = word_tokenize(text)\n",
        "\n",
        "  filtered_text = [w for w in text_tokens if not w in stop_words]\n",
        "\n",
        "  f = \" \".join(filtered_text)\n",
        "\n",
        "  y = []\n",
        "\n",
        "  for i in list(f.split(\" \")):\n",
        "    y.append(stemmer.stem(i))\n",
        "\n",
        "  return \" \".join(y)  "
      ],
      "metadata": {
        "id": "tysHkI3YfmWd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/DA-AI-ML-interview-assignment-Data (1).csv')"
      ],
      "metadata": {
        "id": "AX7Lsl-lfdbC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_ratings'] = convert1(df['num_ratings'])\n",
        "df['num_reviews'] = convert1(df['num_reviews'])\n",
        "df['num_followers'] = convert1(df['num_followers'])\n",
        "\n",
        "df['num_ratings'] = df['num_ratings'] / 1000"
      ],
      "metadata": {
        "id": "um1Ao0cwfddo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Unnamed: 0' , 'genre' , 'num_ratings'] , axis = 1 , inplace = True)"
      ],
      "metadata": {
        "id": "KHOys3lnfdg_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "dff = df[['title', 'name', 'num_reviews','num_followers', 'synopsis']]\n",
        "dff['tags'] = dff['title'] + \" \" + dff['name'] + \" \" + dff['synopsis']\n",
        "\n",
        "df2 = dff[['num_reviews' , 'num_followers', 'tags']]\n",
        "\n",
        "df2['tags'] = df2.tags.apply(data_pre)"
      ],
      "metadata": {
        "id": "qEotbueZfdjz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df2\n",
        "\n",
        "X = pd.concat([X , df['rating']] , axis = 1)"
      ],
      "metadata": {
        "id": "iNfKtrtCfdm0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "iAbQNd3zfdp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "X['vector'] = X['tags'].apply(lambda tags: nlp(tags).vector)\n",
        "\n",
        "xm = np.stack(X['vector'])\n",
        "\n",
        "P = pd.DataFrame(xm)\n",
        "R = pd.concat([X.drop(['tags' , 'vector'] , axis = 1) , P] , axis = 1) "
      ],
      "metadata": {
        "id": "hGOZSDN1fdtE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = R\n",
        "\n",
        "A_train2, A_test2, b_train2, b_test2 = train_test_split(z.drop(['rating'] , axis = 1),z['rating'], test_size=0.20, random_state=42 ,shuffle = True)"
      ],
      "metadata": {
        "id": "4qc9bNHHfdxZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build3_model(model):\n",
        "  model.fit(A_train2 , b_train2)\n",
        "  b_pred2 = model.predict(A_test2)\n",
        "  \n",
        "  print(\"r2_score  :\" , r2_score(b_test2 , b_pred2))\n",
        "  print(\"mean squared error  :\" , mse(b_test2 , b_pred2)) \n",
        "  print(\"root mean squared error  :\" , mse(b_test2 , b_pred2) ** (0.5))\n",
        "  print(\"best parameters  :\" , model.best_params_)\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "id": "z7-ZMHOqfd0x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "\n",
        "model1 = ElasticNet()\n",
        "params = {\n",
        "    'alpha':[1 , 0.5 , 0.05 , 0.7],\n",
        "    'l1_ratio':[0.1 , 0.001 , 0.005 , 0.2 , 0.002]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator = model1 , param_grid = params , scoring = 'r2' , n_jobs = -1 , cv = 5)\n",
        "\n",
        "print(cross_val_score(grid , A_train2 , b_train2 , cv = 5 , scoring = 'r2').mean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hdnG2v5fd5D",
        "outputId": "723af80c-c83b-4d93-e9dd-060e53e1d962"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method mean of numpy.ndarray object at 0x7fba3c139270>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "build3_model(grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzE7U8Bnfd8L",
        "outputId": "dd0b6303-7b55-4bb0-a8cd-453cb9c331a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r2_score  : 0.15906696125727904\n",
            "mean squared error  : 0.05072226389893163\n",
            "root mean squared error  : 0.22521603828087294\n",
            "best parameters  : {'alpha': 0.5, 'l1_ratio': 0.001}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDWhBC8JfelW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_U_k6T1fepw"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}