# -*- coding: utf-8 -*-
"""1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zC68d8XlR9clMtNyKKAKb18G3CXxUcvy
"""

import numpy as np 
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/DA-AI-ML-interview-assignment-Data.csv')

import nltk
nltk.download('stopwords')
nltk.download('punkt')

import re
from nltk.tokenize import word_tokenize

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

df

df.info()

df['name'].value_counts()          ## we observed 842 unique writers

df.describe()

df['genre'].nunique()

"""columns num_ratings,	num_reviews and 	num_followers	are in string format.
- needed to convert into integer format
"""

def convert1(s):
  l = []
  s = list(s)
  for i in s:
    if 'followers' in i:
      i = i.replace(" followers","")
    else:
      i = i  
    if 'k' not in i:
      u = i.replace("," , "")
      u = int(u)
    elif 'k' in i:
      u = float(i[0:len(i) - 1]) * 1000
    else:
      u = i  
    l.append(u)
  return l



df['num_ratings'] = convert1(df['num_ratings'])

df['num_reviews'] = convert1(df['num_reviews'])

df['num_followers'] = convert1(df['num_followers'])

df['num_followers'].iloc[989]

df.describe()

df['num_ratings'] = df['num_ratings'] / 1000

df.describe()

"""analysing numerical columns rating,	num_ratings,	num_reviews and 	num_followers on genre column.

Genre v/s Ratings
"""

plt_1 = plt.figure(figsize=(12, 6))

sns.barplot(x = df['genre'] , y = df['rating'])
plt.show()

"""It has been observed that there is no importance of rating in predicting the genre, we can omit this feature

Genre v/s Number of ratings
"""

plt_1 = plt.figure(figsize=(12, 6))

sns.barplot(x = df['genre'] , y = df['num_ratings'])

"""It has been observed that number of ratings is a key feature in predicting the genre of the book.
- no. of ratings is very high for romance and science fiction genres

Genre v/s Number of reviews
"""

plt_1 = plt.figure(figsize=(12, 6))

sns.barplot(x = df['genre'] , y = df['num_reviews'])

"""The role of Number of reviews in predicting the genre is almost identical to that of  'Number of ratings'

Genre v/s Number of followers
"""

plt_1 = plt.figure(figsize=(12, 6))

sns.barplot(x = df['genre'] , y = df['num_followers'])

"""It has been observed that number of followers is a key feature in predicting the genre of the book.

no. of ratings is very high for horror genre followed by fantasy genre

unknown, ratings, number of reviews features are dropped.
"""

df.columns

dff = df[['title', 'name', 'num_ratings','num_followers', 'synopsis', 'genre']]

dff.head()



df['genre'].value_counts()

df['synopsis'][989]

"""Synopsis provides the best way to identify the genre of the movie.
- found many unnessescary characters in the synopsis
- primary aim to make the tokens out of synopsis at the same time cleaning it
"""

def data_pre(text):
  text = str(text).lower()
  text = re.sub(r"https\S+","",text,flags=re.MULTILINE)

  text = re.sub(r'\@w+|\#',"",text)

  text = re.sub(r'[^\w\s]' , "", text)

  text_tokens = word_tokenize(text)

  filtered_text = [w for w in text_tokens if not w in stop_words]

  f = " ".join(filtered_text)

  y = []

  for i in list(f.split(" ")):
    y.append(stemmer.stem(i))

  return " ".join(y)

data_pre(df['synopsis'][989])

dff['tags'] = dff['title'] + " " + dff['name'] + " " + dff['synopsis']

dff.head()

df2 = dff[['num_ratings' , 'num_followers', 'tags' , 'genre']]

df2.head()

df2['tags'] = df2.tags.apply(data_pre)

"""Train test split before doing countvectorizer or tf idf or glove"""

X = df2.drop(['tags'] , axis = 1)
y = df2['tags']

count_vec = CountVectorizer(min_df = 5)
X = count_vec.fit_transform(df2['tags'].values)

print(count_vec.vocabulary_)

df2['genre'].unique()

maps = {'history':0, 'horror':1, 'psychology':2, 'romance':3, 'science':4,
       'science_fiction':5, 'sports':6, 'thriller':7, 'travel':8, 'fantasy':9}

df2['genre'] = df2['genre'].map(maps)

df2

"""Train test split before doing countvectorizer or tf idf or glove"""

## universal cell

X = df2.drop(['genre'] , axis = 1)
y = df2.genre

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42 ,shuffle = True, stratify = y)

y_train.value_counts()

y_test.value_counts()

count_vec = CountVectorizer(min_df = 5)
vc_train = count_vec.fit_transform(X_train['tags'].values)
vc_test = count_vec.transform(X_test['tags'].values)

a1 = vc_train.toarray()
d1 = pd.DataFrame(a1)

a2 = vc_test.toarray()
d2 = pd.DataFrame(a2)

X_train = X_train.reset_index()
X_test = X_test.reset_index()

j = pd.concat([d1,X_train.drop(['index' ,'tags'] , axis = 1)] , axis = 1)
k = pd.concat([d2,X_test.drop(['index' , 'tags' ] , axis = 1)] , axis = 1)

X_train = j
X_test = k

X_train.shape

X_test.head()





X_train.head()

from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

import matplotlib.pyplot as plt

def build_model(model):
  model.fit(X_train , y_train)
  y_pred = model.predict(X_test)
  
  acc.append(accuracy_score(y_test, y_pred))
  #print()

  cm = confusion_matrix(y_test , y_pred)
  cm_df = pd.DataFrame(data = cm, columns = np.arange(10) , index = np.arange(10))

  sns.heatmap(cm_df , annot = True , cbar = False , fmt='d')

  #plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB

mod = ['LogisticRegression' , 'RandomForestClassifier' , 'DecisionTreeClassifier' , 'XGBClassifier' , 'MultinomialNB']
acc = []

import warnings
warnings.filterwarnings('ignore')

model1 = LogisticRegression(class_weight = 'balanced')
build_model(model1)



model2 = RandomForestClassifier(class_weight = 'balanced')
build_model(model2)

model3 = DecisionTreeClassifier(class_weight = 'balanced')
build_model(model3)

model4 = XGBClassifier()
build_model(model4)

modelnb = MultinomialNB()
build_model(modelnb)

plt_1 = plt.figure(figsize=(12, 6))

plt.xlabel("model")
plt.ylabel("accuracy")
sns.pointplot(mod , acc )
plt.show()







"""Standard scaler applied"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

import warnings
warnings.filterwarnings('ignore')

model5 = KNeighborsClassifier(n_neighbors = 10)
build_model(model5)

import warnings
warnings.filterwarnings('ignore')

model2 = RandomForestClassifier(class_weight = 'balanced')
build_model(model2)

"""TF IDF"""

import warnings
warnings.filterwarnings('ignore')

def buildW_model(model):
  model.fit(X_train2 , y_train2)
  y_pred2 = model.predict(X_test2)

  acc.append(accuracy_score(y_test2, y_pred2))
  
 # print(classification_report(y_test2, y_pred2))
  #print()

  cm = confusion_matrix(y_test2 , y_pred2)
  cm_df = pd.DataFrame(data = cm, columns = np.arange(10) , index = np.arange(10))

  ##sns.heatmap(cm_df , annot = True , cbar = False , fmt='d')

  #plt.show()'''

X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.20, random_state=42 ,shuffle = True, stratify = y)

tf_idf = TfidfVectorizer(min_df = 5)
tf_train = tf_idf.fit_transform(X_train2['tags'].values)
tf_test = tf_idf.transform(X_test2['tags'].values)

a1 = tf_train.toarray()
d1 = pd.DataFrame(a1)

a2 = tf_test.toarray()
d2 = pd.DataFrame(a2)

X_train2 = X_train2.reset_index()
X_test2 = X_test2.reset_index()

j = pd.concat([d1,X_train2.drop(['index' ,'tags'] , axis = 1)] , axis = 1)
k = pd.concat([d2,X_test2.drop(['index' , 'tags' ] , axis = 1)] , axis = 1)

X_train2 = j
X_test2 = k

mod = ['LogisticRegression' , 'RandomForestClassifier' , 'DecisionTreeClassifier' , 'XGBClassifier' , 'MultinomialNB']
acc = []

import warnings
warnings.filterwarnings('ignore')

model1 = LogisticRegression(class_weight = 'balanced')
buildW_model(model1)



model2 = RandomForestClassifier(class_weight = 'balanced')
buildW_model(model2)

model3 = DecisionTreeClassifier(class_weight = 'balanced')
buildW_model(model3)

model4 = XGBClassifier()
buildW_model(model4)

modelnb = MultinomialNB()
buildW_model(modelnb)


plt_1 = plt.figure(figsize=(12, 6))

plt.xlabel("model")
plt.ylabel("accuracy")
sns.pointplot(mod , acc )
plt.show()

"""surprisingly the logistic regression performed top notch when TF-IDF has been used as vectorizer.
- xgb out performed the random forest and descsio trees
"""

import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import MinMaxScaler
scaler2 = MinMaxScaler()

X_train2 = scaler2.fit_transform(X_train2)
X_test2 = scaler2.transform(X_test2)

modelnb = MultinomialNB()
build_model(modelnb)

import warnings
warnings.filterwarnings('ignore')

model1 = LogisticRegression(class_weight = 'balanced')
build_model(model1)

"""using spacy"""

!pip install -U pip setuptools wheel
!pip install -U spacy
!python -m spacy download en_core_web_lg

import spacy
nlp = spacy.load("en_core_web_lg")
X['vector'] = X['tags'].apply(lambda tags: nlp(tags).vector)

sd = pd.DataFrame(X['vector'])

X

xm = np.stack(X['vector'])

P = pd.DataFrame(xm)

P



R = pd.concat([X.drop(['tags' , 'vector'] , axis = 1) , P] , axis = 1)

R

X_train3, X_test3, y_train3, y_test3 = train_test_split(X.drop(['tags' , 'vector'] , axis = 1), y, test_size=0.20, random_state=42 ,shuffle = True, stratify = y)

import warnings
warnings.filterwarnings('ignore')

def build_model3(model):
  model.fit(X_train3 , y_train3)
  y_pred3 = model.predict(X_test3)
  
  acc.append(accuracy_score(y_test3, y_pred3))
  
 # print(classification_report(y_test2, y_pred2))
  #print()

  cm = confusion_matrix(y_test2 , y_pred2)
  cm_df = pd.DataFrame(data = cm, columns = np.arange(10) , index = np.arange(10))

  ##sns.heatmap(cm_df , annot = True , cbar = False , fmt='d')

  #plt.show()'''

mod = ['LogisticRegression' , 'RandomForestClassifier' , 'DecisionTreeClassifier' , 'XGBClassifier']
acc = []

import warnings
warnings.filterwarnings('ignore')

model1 = LogisticRegression(class_weight = 'balanced')
build_model(model1)



model2 = RandomForestClassifier(class_weight = 'balanced')
build_model(model2)

model3 = DecisionTreeClassifier(class_weight = 'balanced')
build_model(model3)

model4 = XGBClassifier()
build_model(model4)


plt_1 = plt.figure(figsize=(12, 6))

plt.xlabel("model")
plt.ylabel("accuracy")
sns.pointplot(mod , acc )
plt.show()



"""spacy word embedding performing worse than count vectorizer and tf idf

Lets balance the genre class :)
"""



X = X.drop(['vector'] , axis = 1)

M = pd.concat([X , y] , axis = 1)

M

count_vec = CountVectorizer(min_df = 5)
vc = count_vec.fit_transform(X['tags'].values)

a1 = vc.toarray()
d1 = pd.DataFrame(a1)

j = pd.concat([d1,M.drop(['tags'] , axis = 1)] , axis = 1)

j

from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='minority')
#X1 = j.drop(['genre'] , axis = 1)
#y1 = j['genre']

X1 , y1 = smote.fit_resample(X1 , y1)

y1.value_counts()

"""multi class smote"""

j['genre'].value_counts()

z = j[j['genre'] == 7]

for i in range(0,10):

  if i != 7:

    p = pd.concat([j[j['genre'] == 7] , j[j['genre'] == i]] , axis = 0)

    X1 = p.drop(['genre'] , axis = 1)
    y1 = p['genre']
    X1 , y1 = smote.fit_resample(X1 , y1)

    o = pd.concat([X1 , y1] , axis = 1)
    o = o[o['genre'] == i]

    z =  pd.concat([o , z] , axis = 0)

z['genre'].value_counts()

z = z.reset_index()

z = z.drop(['index'] , axis = 1)

z.shape

z.describe()

c = z.drop(['genre'] , axis = 1)
v = z['genre']

c.shape

v.shape

A_train, A_test, b_train, b_test = train_test_split(c,v, test_size=0.20, random_state=42 ,shuffle = True)

z.shape



import warnings
warnings.filterwarnings('ignore')

def buildK_model(model):
  model.fit(A_train , b_train)
  b_pred = model.predict(A_test)
  
  acc.append(accuracy_score(b_test, b_pred))
  
 # print(classification_report(y_test2, y_pred2))
  #print()

  #cm = confusion_matrix(y_test2 , y_pred2)
  #cm_df = pd.DataFrame(data = cm, columns = np.arange(10) , index = np.arange(10))

  ##sns.heatmap(cm_df , annot = True , cbar = False , fmt='d')

  #plt.show()'''

mod = ['LogisticRegression' , 'RandomForestClassifier' , 'DecisionTreeClassifier' , 'XGBClassifier' , 'MultinomialNB']
acc = []

import warnings
warnings.filterwarnings('ignore')

model1 = LogisticRegression(class_weight = 'balanced')
buildK_model(model1)



model2 = RandomForestClassifier(class_weight = 'balanced')
buildK_model(model2)

model3 = DecisionTreeClassifier(class_weight = 'balanced')
buildK_model(model3)

model4 = XGBClassifier()
buildK_model(model4)

modelnb = MultinomialNB()
buildK_model(modelnb)


plt_1 = plt.figure(figsize=(12, 6))

plt.xlabel("model")
plt.ylabel("accuracy")
sns.pointplot(mod , acc )
plt.show()

for i in range(5):
  print(mod[i] , acc[i])

"""random forest worked extremely well compared to all other

tf idf and multi class smote
"""

tf_vec = TfidfVectorizer(min_df = 5)
tf = tf_vec.fit_transform(X['tags'].values)

a1 = tf.toarray()
d1 = pd.DataFrame(a1)

k = pd.concat([d1,M.drop(['tags'] , axis = 1)] , axis = 1)

k

z = k[k['genre'] == 7]
j = k

for i in range(0,10):

  if i != 7:

    p = pd.concat([j[j['genre'] == 7] , j[j['genre'] == i]] , axis = 0)

    X1 = p.drop(['genre'] , axis = 1)
    y1 = p['genre']
    X1 , y1 = smote.fit_resample(X1 , y1)

    o = pd.concat([X1 , y1] , axis = 1)
    o = o[o['genre'] == i]

    z =  pd.concat([o , z] , axis = 0)

z

A_train1, A_test1, b_train1, b_test1 = train_test_split(z.drop(['genre'] , axis = 1),z['genre'], test_size=0.20, random_state=42 ,shuffle = True)

b_train1.value_counts()

import warnings
warnings.filterwarnings('ignore')

def buildD_model(model):
  model.fit(A_train1 , b_train1)
  b_pred1 = model.predict(A_test1)
  
  acc.append(accuracy_score(b_test1, b_pred1))
  
 # print(classification_report(y_test2, y_pred2))
  #print()

  #cm = confusion_matrix(y_test2 , y_pred2)
  #cm_df = pd.DataFrame(data = cm, columns = np.arange(10) , index = np.arange(10))

  ##sns.heatmap(cm_df , annot = True , cbar = False , fmt='d')

  #plt.show()'''

mod = ['LogisticRegression' , 'RandomForestClassifier' , 'DecisionTreeClassifier' , 'XGBClassifier' , 'MultinomialNB']
acc = []

import warnings
warnings.filterwarnings('ignore')

model1 = LogisticRegression(class_weight = 'balanced')
buildD_model(model1)



model2 = RandomForestClassifier(class_weight = 'balanced')
buildD_model(model2)

model3 = DecisionTreeClassifier(class_weight = 'balanced')
buildD_model(model3)

model4 = XGBClassifier()
buildD_model(model4)

modelnb = MultinomialNB()
buildD_model(modelnb)


plt_1 = plt.figure(figsize=(12, 6))

plt.xlabel("model")
plt.ylabel("accuracy")
sns.pointplot(mod , acc )
plt.show()

for i in range(5):
  print(mod[i] , acc[i])

"""spacy"""

X

X['vector'] = X['tags'].apply(lambda tags: nlp(tags).vector)

xm = np.stack(X['vector'])

P = pd.DataFrame(xm)
R = pd.concat([X.drop(['tags' , 'vector'] , axis = 1) , P] , axis = 1)

A_train2, A_test2, b_train2, b_test2 = train_test_split(z.drop(['genre'] , axis = 1),z['genre'], test_size=0.20, random_state=42 ,shuffle = True)

import warnings
warnings.filterwarnings('ignore')

def build3_model(model):
  model.fit(A_train2 , b_train2)
  b_pred2 = model.predict(A_test2)
  
  acc.append(accuracy_score(b_test2, b_pred2))
  
 # print(classification_report(y_test2, y_pred2))
  #print()

  #cm = confusion_matrix(y_test2 , y_pred2)
  #cm_df = pd.DataFrame(data = cm, columns = np.arange(10) , index = np.arange(10))

  ##sns.heatmap(cm_df , annot = True , cbar = False , fmt='d')

  #plt.show()'''

mod = ['LogisticRegression' , 'RandomForestClassifier' , 'DecisionTreeClassifier' , 'XGBClassifier' ]
acc = []

import warnings
warnings.filterwarnings('ignore')

model1 = LogisticRegression(class_weight = 'balanced')
build3_model(model1)



model2 = RandomForestClassifier(class_weight = 'balanced')
build3_model(model2)

model3 = DecisionTreeClassifier(class_weight = 'balanced')
build3_model(model3)

model4 = XGBClassifier()
build3_model(model4)


plt_1 = plt.figure(figsize=(12, 6))

plt.xlabel("model")
plt.ylabel("accuracy")
sns.pointplot(mod , acc )
plt.show()

for i in range(4):
  print(mod[i] , acc[i])

